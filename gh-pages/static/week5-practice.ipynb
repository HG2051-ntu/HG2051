{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # make sure NLTK is installed and loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Lists\n",
    "\n",
    "Use the `nltk.corpus.words` wordlist to estimate the following for several text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import gutenberg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose a variety of texts from the Gutenberg corpus. What percentage of the texts' vocabularies are not in the wordlist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg.fileids()  # inspect available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['austen-emma.txt', 'milton-paradise.txt', 'shakespeare-macbeth.txt']  # choose some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WORDS = set(words.words())  # words.words() is a list; make it a set to speed up lookup\n",
    "for file in files:\n",
    "    # Get the list of words in the text and normalize early.\n",
    "    # The isalpha() filter removes tokens like \",\", but also valid words with punctuation.\n",
    "    # How you normalize and filter is up to you.\n",
    "    textwords = [w.lower() for w in gutenberg.words(file) if w.isalpha()]\n",
    "    # Words not in the set of \"known\" words are often called OOV (out-of-vocabulary)\n",
    "    oov = [w for w in textwords if w not in WORDS]\n",
    "    print(file)\n",
    "    print('  sample:', oov[:10])\n",
    "    print('  percent of tokens that are unknown:', 100 * len(oov) / len(textwords))\n",
    "    print('  percent of types that are unknown:', 100 * len(set(oov)) / len(set(textwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What percentage of the wordlist are present in the texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # Get the list of words, as before.\n",
    "    textwords = [w.lower() for w in gutenberg.words(file) if w.isalpha()]\n",
    "    # IV = in-vocabulary, by analogy to OOV\n",
    "    iv = [w for w in textwords if w in WORDS]\n",
    "    print(file)\n",
    "    print('  percent of wordlist present in text:', 100 * len(set(iv)) / len(WORDS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU Pronouncing Dictionary\n",
    "\n",
    "Use the ARPABET transcriptions in the `nltk.corpus.cmudict` corpus to investigate sound patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu = cmudict.dict()\n",
    "cmu['pronounce']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick some minimal pairs and look at vowel differences (e.g., *pick* / *pack* / *peck* / *peak*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in ('pick', 'pack', 'peck', 'peak'):\n",
    "    print(word, '->', cmu[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Devise a function for identifying rhyming words (how they are identified is up to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rhymes(word1, word2):\n",
    "    pron_list1 = cmu[word1]\n",
    "    pron_list2 = cmu[word2]\n",
    "    return any(p1[-2:] == p2[-2:]  # are the last 2 phonemes enough? are they too much?\n",
    "               for p1 in pron_list1\n",
    "               for p2 in pron_list2)\n",
    "\n",
    "print('rhymes with \"pack\":')\n",
    "for other in ('pick', 'peck', 'peak', 'back', 'track'):\n",
    "    print('  ', other, rhymes('pack', other))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (extra) why doesn't something like \"smokestack\" or \"quarterback\" rhyme with \"pack\" according to the function above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in ('pack', 'smokestack', 'quarterback'):\n",
    "    print(word, cmu[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the largest clusters of rhyming words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first create a structure mapping each rhyming scheme to the list of words ending in the scheme\n",
    "clusters = {}\n",
    "for word, pron_list in cmu.items():\n",
    "    for pron in pron_list:\n",
    "        scheme = tuple(pron[-2:])  # make it a tuple so it can be a dictionary key\n",
    "        # initialize an empty list if we haven't seem the rhyming scheme before\n",
    "        if scheme not in clusters:\n",
    "            clusters[scheme] = []\n",
    "        clusters[scheme].append(word)\n",
    "\n",
    "# To find the largets cluster, we could go through each and keep track of the largest we've seen:\n",
    "max_scheme = None\n",
    "max_value = 0\n",
    "for scheme, cluster in clusters.items():\n",
    "    if len(cluster) > max_value:\n",
    "        max_scheme = scheme\n",
    "        max_value = len(cluster)\n",
    "print('largest cluster')\n",
    "print('  scheme:', max_scheme)\n",
    "print('  size:', len(clusters[max_scheme]))\n",
    "\n",
    "# Alternatively, use the max() function with a \"lambda\" expression (like an inline function)\n",
    "max_scheme = max(clusters, key=lambda scheme: len(clusters[scheme]))\n",
    "print('largest cluster')\n",
    "print('  scheme:', max_scheme)\n",
    "print('  size:', len(clusters[max_scheme]))\n",
    "\n",
    "print('sample:', clusters[max_scheme][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "Use `nltk.corpus.wordnet` to look at word relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the synsets of *student*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('student')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the definition of each synset of student?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('student'):\n",
    "    print(synset, synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the **hyponyms** of each synset of *student*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('student'):\n",
    "    print(synset, synset.hyponyms())\n",
    "    print()  # a blank line in between helps; these are long lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many synsets are avaiable for each of *professor*, *lecturer*, *instructor*, and *teacher*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = ('professor', 'lecturer', 'instructor', 'teacher')\n",
    "for word in words:\n",
    "    print(word, len(wn.synsets(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any overlapping synsets among them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the direction of the pairing (e.g., ('professor', 'lecturer') or ('lecturer', 'professor')\n",
    "# doesn't matter, so we'll avoid that with a slice in the second for-loop)\n",
    "for i, word1 in enumerate(words):\n",
    "    for word2 in words[i+1:]:\n",
    "        print(word1, word2, set(wn.synsets(word1)).intersection(wn.synsets(word2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the `lowest_common_hypernyms()` method on synsets to find what is the shared **hypernym** of *student* and *professor*. How about *professor* and *lecturer*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this I'll pick 'student.n.01' and 'professor.n.01'\n",
    "wn.synset('student.n.01').lowest_common_hypernyms(wn.synset('professor.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this I'll pick 'professor.n.01' and 'lector.n.02'\n",
    "wn.synset('professor.n.01').lowest_common_hypernyms(wn.synset('lector.n.02'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The synsets retrieved from WordNet are generally sorted by the frequency of occurrence (bonus question: how would the \"frequency of occurrence\" be computed?). Write a function that tags each word in a sentence with the first synset returned by WordNet. Skip words that do not return any synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For bonus question: we need a corpus that has been annotated with word senses\n",
    "# to determine frequency *for that corpus*. The creators of the wordnet can also\n",
    "# order the synsets to encode general preference.\n",
    "def sense_tag(sentence):\n",
    "    words = sentence.split()\n",
    "    pairs = []\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        if synsets:\n",
    "            pairs.append((word, synsets[0]))  # only the first synset\n",
    "    return pairs\n",
    "\n",
    "print(sense_tag('I bought a mouse for my laptop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider these sentences:\n",
    "  - *The doctor is in, today.*\n",
    "  - *The doctor is in the office, today.*\n",
    "  - *The doctor's shoes are very in, this season.*\n",
    "\n",
    "**Q:** With your sysent tagger, do all sentences get the same synset for *in*? Which sysnets should they get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sense_tag('The doctor is in , today .'))  # spaces around punctuation because my tokenization is just split()\n",
    "print(sense_tag('The doctor is in the office, today .'))\n",
    "print(sense_tag(\"The doctor's shoes are very in , this season .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('in'):\n",
    "    print(synset, synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all get the same but it's correct for any. They should get:\n",
    "- in.s.01\n",
    "- None\n",
    "- in.s.03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Can you think of ways we might improve the tagger to get better performance?\n",
    "\n",
    "**A:** <details><summary></summary>If we had the part-of-speech information for each word, this could help wordnet choose the correct synset more often. We could also try to build statistical models, for instance by looking at each word's left and right context to help decide, but this requires annotated data to train the model.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How would we measure if the performance improves or degrades?\n",
    "\n",
    "**A:** <details><summary></summary>We need some **gold standard** annotations to compare against. Without those, we would have to manually determine, with our own intuition, whether an annotation is correct or not, and this has issues with consistency and scale.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "859938ca7b6a8471053e30db1fb8d68e922ac19bedf325ea06091edab2c59131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
